{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "## preprocessing\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "current_pth = os.path.abspath('../')\n",
    "sys.path.append(current_pth)\n",
    "from utils.dataset import validate_random_partitions,generate_partition_datatensor\n",
    "from utils.sequence_process import SeqProcessConfig, HaplotypeSeqProcessor,validate_df\n",
    "from utils.data_preprocess import get_char, drop_nan_missing_values,drop_wilde_type,renormalize,add_absolute_efficiency\n",
    "from utils.data_preprocess import get_train_test_val,transform_genseq_upper\n",
    "from utils.utilities import ReaderWriter\n",
    "from utils.data_preprocess import create_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pth = os.path.abspath('../../../crispr_private')\n",
    "data_dir = create_directory(os.path.join(data_pth, 'dataset', 'final_dataset'))\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1. read the data\n",
    "editor_name = 'ABEmax-SpRY' #'ABEmax-SpRY' 'ABE8e-NG', 'ABE8e-SpRY', 'ABE8e-SpCas9','ABEmax-NG','ABEmax-SpCas9','ABEmax-SpRY'\n",
    "input_type = 'protospacer_PAM'  #'protospacer','protospacer_PAM','protospacer_PAM_overhangs'\n",
    "k = 5  # how many position in the overhangs we consider\n",
    "\n",
    "\n",
    "exp_name = 'Liver_SBApproach' #'Liver_LentiAAV'       # 'Liver_LentiAAV'\n",
    "\n",
    "'''\n",
    "if editor_name == 'ABEmax-SpRY':\n",
    "    print('Loading dataset ', editor_name )\n",
    "    df = pd.read_csv(os.path.join(data_dir,'HEK_BLDLib_10d_'f'{editor_name}_1_Seq1_ProportionTable_merged.txt'),\n",
    "                 header=0, \n",
    "                 delimiter='\\t')\n",
    "else:\n",
    "    print('Loading dataset ', editor_name )\n",
    "    df = pd.read_csv(os.path.join(data_dir,'HEK_BLDLib_10d_'f'{editor_name}_1_Merged_ProportionTable_merged.txt'),\n",
    "                 header=0, \n",
    "                 delimiter='\\t')\n",
    "'''\n",
    "#editor_name = 'ABE8e-SpCas9'\n",
    "df = pd.read_csv(os.path.join(data_dir,'merged_invivo',f'{exp_name}_'f'{editor_name}_2_Seq1_ProportionTable_merged.txt'),\n",
    "                 header=0, \n",
    "                 delimiter='\\t')\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['refSeq'] =='CAGCCAAGCCCACGGCTACC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'rname':'ID'}, inplace=True)\n",
    "df.rename(columns={'refSeq':'Reference'}, inplace=True)\n",
    "df.rename(columns={'seq':'Outcome'}, inplace=True)\n",
    "df.rename(columns={'refSeq_pam':'PAM'}, inplace=True)\n",
    "df['Proportion'] = df['genoCount']/df['allCount']\n",
    "N1 = len(df)\n",
    "print(df.duplicated(['Outcome', 'Reference']).sum())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Reference'] =='CAGCCAAGCCCACGGCTACC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_to_remove = ['NM_006920.6(SCN1A)c.4873CtoT(p.Arg1625Ter)Position5', \n",
    "               'NM_006920.6(SCN1A)c.4778GtoA(p.Trp1593Ter)Position3',\n",
    "                'NM_001040142.2(SCN2A)c.4876CtoT(p.Arg1626Ter)Position8',\n",
    "                'NM_006920.6(SCN1A)c.4873CtoT(p.Arg1625Ter)Position6',\n",
    "                'NM_001040142.2(SCN2A)c.4876CtoT(p.Arg1626Ter)Position8',\n",
    "                'NM_006920.6(SCN1A)c.4778GtoA(p.Trp1593Ter)Position4',\n",
    "                'NM_006920.6(SCN1A)c.4873CtoT(p.Arg1625Ter)Position9',\n",
    "                'NM_002977.3(SCN9A)c.4700GtoA(p.Trp1567Ter)Position5',\n",
    "               'NM_001040142.2(SCN2A)c.4876CtoT(p.Arg1626Ter)Position7',\n",
    "                'NM_006920.6(SCN1A)c.4778GtoA(p.Trp1593Ter)Position4'\n",
    "               \n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[~df['ID'].isin(ID_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[filtered_df['Reference'] =='CAGCCAAGCCCACGGCTACC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_df['Reference'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_df['ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filtered_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated(['Outcome', 'Reference']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. clean the data\n",
    "df = drop_nan_missing_values(df)\n",
    "print(df['Proportion'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)/len(df['ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['PAM'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### introduce cut off and renormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_pro = df.groupby(by='Reference',group_keys=False)\n",
    "pbar = tqdm(total=df_pro.ngroups)\n",
    "prg_counter = 0\n",
    "processed_df = df_pro.apply(filter_edit_window,pbar )\n",
    "processed_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### introduce  efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##3. add two clomuns for absolute efficiency score and socre for wild type\n",
    "prg_counter=0\n",
    "dfg = df.groupby(by='Reference') \n",
    "pbar = tqdm(total=dfg.ngroups)\n",
    "df_clean = dfg.apply(add_absolute_efficiency, pbar,prg_counter)\n",
    "pbar.close()\n",
    "df_clean.reset_index(inplace=True, drop=True)\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_clean[df_clean['wild_type'] ==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##4. drop the sequences that has only wild type\n",
    "filtered_df = df_clean.drop(df_clean[df_clean['wild_type'] ==1].index)\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_df['ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['Proportion'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_df)/len(filtered_df['ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_df['PAM'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define column names pointers to use throughout, tseq_col and outcome_col would be modified if we need to use diffferent input outputs\n",
    "id_col = 'ID' # sequnce id column name\n",
    "outcome_prop_col = ['Proportion','absolute_efficiency']  # propotion of edits column name  \n",
    "df = filtered_df.copy()\n",
    "\n",
    "\n",
    "if input_type == 'protospacer':\n",
    "    extended_df = df\n",
    "    tseq_col = 'Reference' # target sequence (wild-type) column name\n",
    "    outcome_col = 'Outcome' # edit sequence (i.e. edited outcome sequence) column name  \n",
    "    \n",
    "elif input_type == 'protospacer_PAM':\n",
    "    protospacer_PAM =pd.DataFrame(df['Reference'] + df['PAM'])\n",
    "    protospacer_PAM.columns = ['protospacer_PAM']\n",
    "    output_PAM =pd.DataFrame(df['Outcome'] + df['PAM'])\n",
    "    output_PAM.columns = ['Outcome_PAM']\n",
    "    extended_df = pd.concat([df,protospacer_PAM, output_PAM],axis=1)\n",
    "    tseq_col = 'protospacer_PAM' # target sequence (wild-type) column name\n",
    "    outcome_col = 'Outcome_PAM' # edit sequence (i.e. edited outcome sequence) column name \n",
    "    \n",
    "    \n",
    "elif input_type == 'protospacer_PAM_overhangs':\n",
    "\n",
    "    df_left = pd.DataFrame({'refseq_leftoverhang': df['refseq_leftoverhang'].str[-k:]})\n",
    "    df_right = pd.DataFrame({'refseq_rightoverhang': df['refseq_rightoverhang'].str[:k]})\n",
    "    protospacer_PAM =pd.DataFrame(df_left['refseq_leftoverhang']+ df['Reference'] + df['PAM']+ df_right['refseq_rightoverhang'])\n",
    "    protospacer_PAM.columns = ['protospacer_PAM_overhangs']\n",
    "    output_PAM =pd.DataFrame(df_left['refseq_leftoverhang']+ df['Outcome'] + df['PAM']+df_right['refseq_rightoverhang'])\n",
    "    output_PAM.columns = ['Outcome_PAM_overhangs']\n",
    "    extended_df = pd.concat([df,protospacer_PAM, output_PAM],axis=1)\n",
    "    tseq_col = 'protospacer_PAM_overhangs' # target sequence (wild-type) column name\n",
    "    outcome_col = 'Outcome_PAM_overhangs' # edit sequence (i.e. edited outcome sequence) column name\n",
    "    \n",
    "else:\n",
    "    print('specify the input type')\n",
    "\n",
    "print(tseq_col)\n",
    "print(outcome_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. prepare the data for proportion model\n",
    "gdf = extended_df.groupby(by='Reference',group_keys=False)\n",
    "pbar = tqdm(total=gdf.ngroups)\n",
    "proportion_df = gdf.apply(drop_wilde_type, pbar)\n",
    "proportion_df.reset_index(inplace=True, drop=True)\n",
    "## renormalize \n",
    "dfg = proportion_df.groupby(by='Reference',group_keys=False)\n",
    "normalized_proportion_df = dfg.apply(renormalize)\n",
    "normalized_proportion_df.reset_index(inplace=True, drop=True)\n",
    "proportion_df = normalized_proportion_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(proportion_df['ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. prepare the data for absolute efficiency model\n",
    "absolute_efficiency_df=extended_df.groupby('Reference').first().reset_index()\n",
    "len(absolute_efficiency_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extended_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(proportion_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_df = transform_genseq_upper(proportion_df, ['Outcome', 'Reference'])\n",
    "assert proportion_df.duplicated(['Outcome', 'Reference']).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(proportion_df[proportion_df.duplicated(['Outcome', 'Reference'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_conv_nucl = {'ABEmax-NG':('A', 'G'),'ABE8e-NG':('A', 'G'), 'ABE8e-SpCas9':('A', 'G'), 'ABE8e-SpRY':('A', 'G'), 'ABEmax-SpCas9':('A','G'),'ABEmax-SpRY':('A', 'G'),}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define sequence processor config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sequence processor config\n",
    "seqconfig = SeqProcessConfig(20, (1,20), (1,20), 1)\n",
    "seq_processor = HaplotypeSeqProcessor(editor_name, target_conv_nucl[editor_name], seqconfig)\n",
    "## removing duplicates (in terms of output sequence)\n",
    "df = seq_processor.preprocess_df(proportion_df, ['ID', 'Reference'], 'Outcome')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_df = seq_processor.process_inp_outp_df(proportion_df, id_col, tseq_col, outcome_col, outcome_prop_col[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create haplotype (i.e. bystander) datatensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.dataset import HaplotypeDataTensor\n",
    "hap_dtensor = HaplotypeDataTensor(seqconfig)\n",
    "hap_dtensor.generate_tensor_from_df(proc_df, target_conv_nucl[editor_name], outcome_prop_col[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "from utils.data_preprocess import VizInpOutp_Haplotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seqconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tindx=proc_df.iloc[90]['seq_id']\n",
    "HTML(VizInpOutp_Haplotype().viz_align_haplotype(proc_df, tindx, outcome_prop_col[0], seqconfig, ('A', 'G')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df['Reference'].unique()) == len(hap_dtensor.indx_seqid_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_assign_df = pd.DataFrame(hap_dtensor.indx_seqid_map.values())\n",
    "seq_assign_df.columns = ['seq_id', 'Input_seq']\n",
    "display(seq_assign_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df.copy()\n",
    "temp.drop_duplicates(subset=['Reference'], ignore_index=True, inplace=True, keep='first')\n",
    "validate_df(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqid_info  = pd.merge(left=seq_assign_df,\n",
    "                       right=temp,\n",
    "                       how='left', \n",
    "                       left_on=['seq_id'], \n",
    "                       right_on=['ID'])\n",
    "\n",
    "display(seqid_info)\n",
    "#display(seqid_info['seq_type'].value_counts())\n",
    "print('-'*15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 3 # number of folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpartitions = get_train_test_val(seqid_info,  run_num = num_splits, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_random_partitions(dpartitions, seqid_info.index.tolist(), val_set_portion=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get datatensor partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtensor_partitions = generate_partition_datatensor(hap_dtensor, dpartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtensor_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tseq_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generate data for the absolute efficiency model\n",
    "y = np.array(seqid_info['absolute_efficiency'])\n",
    "y = y.astype(np.float32)\n",
    "ID = seqid_info['ID']\n",
    "protospacer = seqid_info[tseq_col].apply(get_char)\n",
    "num_nucl = len(protospacer.columns)\n",
    "protospacer.columns = [f'B_encoded{i}' for  i in range(1, num_nucl+1)]\n",
    "protospacer.replace(['A', 'C', 'T', 'G'], [0,1,2,3], inplace=True)\n",
    "x_protospacer = np.array(protospacer)\n",
    "processed_data = [x_protospacer, ID ,y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f'{editor_name}_proportions_encenc_two_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_dir = create_directory(os.path.join(data_dir,'invivo',exp_name, input_type, fname))\n",
    "target_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_dir = create_directory(os.path.join(data_dir, input_type, fname))\n",
    "#target_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtensor_partitions, dpartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump partitions and dtensor\n",
    "ReaderWriter.dump_data(dpartitions, os.path.join(target_dir, f'data_partitions.pkl'))\n",
    "ReaderWriter.dump_data(hap_dtensor, os.path.join(target_dir, f'hap_dtensor.torch'))\n",
    "ReaderWriter.dump_data(dtensor_partitions, os.path.join(target_dir, f'dtensor_partitions.torch'))\n",
    "ReaderWriter.dump_data(processed_data, os.path.join(target_dir, 'list_of_x_f_y.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('finished processing', editor_name, input_type, 'number of partitions', num_splits)\n",
    "print('saving in', target_dir )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
